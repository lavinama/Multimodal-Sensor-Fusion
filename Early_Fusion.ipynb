{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["XGlg7uxki1Cq","1rbhQKptg-uK"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/Jeremy26/visual_fusion_course/blob/main/Visual_Fusion_Early_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"6V2BxA_xar00"},"source":["# Welcome to the Early Fusion Project\n","\n","Before we start, acknowledgement to this repo: https://github.com/kuixu/kitti_object_vis. This course has been based on this repo after seeing the great results and code! <p>\n","\n","We'll use the [KITTI Dataset](http://www.cvlibs.net/datasets/kitti/setup.php) to collect the Point Clouds, Images, and Calibration parameters. <p>\n","\n","After loading data from the dataset, our Early fusion process will happen in 3 steps:\n","1.   **Project the Point Clouds (3D) to the Image(2D)** \n","2.   **Detect Obstacles in 2D** (Camera)\n","3.   **Fuse the Results**\n","\n","Are you ready? ‚úåüèº"]},{"cell_type":"markdown","metadata":{"id":"XGlg7uxki1Cq"},"source":["##0 - Load the Data and Visualize it!"]},{"cell_type":"markdown","metadata":{"id":"-RYckDyji8H1"},"source":["### Link Google Colab to Google Drive"]},{"cell_type":"code","metadata":{"id":"tk-izanQH9iY"},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","work_dir = \"/content/drive/My Drive/Autonomous Projects/visual_fusion\"\n","os.chdir(work_dir)\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vrs7OD_Vi_Zs"},"source":["### Import the necessary libraries"]},{"cell_type":"code","metadata":{"id":"pppZPZ3_jBKN"},"source":["!pip install open3d==0.12.0 # Version 12"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BC_e1h8MICKX"},"source":["import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import glob\n","import open3d as o3d"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wr79FMSMpT8E"},"source":["### Load the Files"]},{"cell_type":"code","metadata":{"id":"qP3-j5OAKhT2"},"source":["image_files = sorted(glob.glob(work_dir + \"/data/img/*.png\"))\n","point_files = sorted(glob.glob(work_dir + \"/data/velodyne/*.pcd\"))\n","label_files = sorted(glob.glob(work_dir + \"/data/label/*.txt\"))\n","calib_files = sorted(glob.glob(work_dir + \"/data/calib/*.txt\"))\n","\n","index = 0\n","pcd_file = point_files[index]\n","image = cv2.cvtColor(cv2.imread(image_files[index]), cv2.COLOR_BGR2RGB)\n","cloud = o3d.io.read_point_cloud(pcd_file)\n","points= np.asarray(cloud.points)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PpMXag26GYwp"},"source":["print(points[2,:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1rbhQKptg-uK"},"source":["### Optional - If your LiDAR file is in binary extension '.bin', use this piece of code to turn it into a '.pcd' and save it"]},{"cell_type":"code","metadata":{"id":"Zq7lPkvYPXow"},"source":["## BIN TO PCD\n","import numpy as np\n","import struct\n","\n","LIDAR_BIN = False\n","\n","if LIDAR_BIN:\n","    size_float = 4\n","    list_pcd = []\n","\n","    file_to_open = point_files[index]\n","    file_to_save = str(point_files[index])[:-3]+\"pcd\"\n","    with open (file_to_open, \"rb\") as f:\n","        byte = f.read(size_float*4)\n","        while byte:\n","            x,y,z,intensity = struct.unpack(\"ffff\", byte)\n","            list_pcd.append([x, y, z])\n","            byte = f.read(size_float*4)\n","    np_pcd = np.asarray(list_pcd)\n","    pcd = o3d.geometry.PointCloud()\n","    v3d = o3d.utility.Vector3dVector\n","    pcd.points = v3d(np_pcd)\n","\n","    o3d.io.write_point_cloud(file_to_save, pcd)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"00YaQZjrjRpW"},"source":["### Visualize the Image"]},{"cell_type":"code","metadata":{"id":"YnSsfviYjTxb"},"source":["f, (ax1) = plt.subplots(1, 1, figsize=(20,10))\n","ax1.imshow(image)\n","ax1.set_title('Image', fontsize=30)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tiNl7OdgjUri"},"source":["### Visualize the Point Clouds"]},{"cell_type":"code","metadata":{"id":"VFRpDWNXjazV"},"source":["!pip install pypotree #https://github.com/centreborelli/pypotree"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_HqRVOwxJP8R"},"source":["import pypotree \n","cloudpath = pypotree.generate_cloud_for_display(points)\n","pypotree.display_cloud_colab(cloudpath)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1 - Project the Points in the Image <p>"],"metadata":{"id":"JLUPDRuhl75v"}},{"cell_type":"markdown","metadata":{"id":"44-hPZXmTulS"},"source":["That part is possibly the hardest to understand and will require your full attention. We want to project the 3D points into the image.<p>\n","\n","It means we'll need to: <p>\n","\n","*   Select the Point that are **visible** in the image ü§î\n","*   Convert the Points **from the LiDAR frame to the Camera Frame** ü§Ø\n","*   Find a way to project **from the Camera Frame to the Image Frame** üò≠\n","\n","<p>\n","No worries here, we'll figure out everything together.\n"]},{"cell_type":"markdown","source":["### 1.1 - Read the Calibration File"],"metadata":{"id":"z0RWpAXCbMbH"}},{"cell_type":"markdown","metadata":{"id":"aTqtmQJoBx-8"},"source":["The first step is to read the calibration files. For each image, we have an associated calibration file that states:<p>\n","\n","\n","*   The instrinsic and extrinsic camera calibration parameters\n","*   The velodyne to camera matrices\n","*   All the other \"sensor A\" to \"sensor B\" matrices\n","<p>\n","They are made from this setup:<p>\n","\n","![link text](http://www.cvlibs.net/datasets/kitti/images/setup_top_view.png)\n","\n","Not everything matters to us here, only a few things:\n","*    **Velo-To-Cam is a variable we'll call V2C** -- It gives the rotation and translation matrices from the Velodyne to the Left Grayscale camera.\n","*    **R0_rect used in Stereo Vision to make the images co-planar.**\n","*   **P2 is the matrix obtained after camera calibration**. It contains the intrinsic matrix K and the extrinsic.\n"]},{"cell_type":"markdown","source":["$P_i$: intrinsic camera calibration matrix for camera $i$\n","\n","$P_0$ and $P_1$ are greyscale\n","$P_2$ and $P_3$ are colour\n","\n","$R_0{rect}$: stereo rectification matrix\n","\n","$Tr_{velo\\ to \\ cam}$: velodyne to camera matrix\n","\n","$Tr_{imu\\ to \\ velo}$: IMU to velodyne matrix"],"metadata":{"id":"BNst_VuSGMTx"}},{"cell_type":"code","metadata":{"id":"fSzz_3tf0xlX"},"source":["class LiDAR2Camera(object):\n","    def __init__(self, calib_file):\n","        calibs = self.read_calib_file(calib_file)\n","        self.calibs = calibs\n","        self.P = self.calibs[\"P2\"].reshape(3, 4)\n","        # print(\"P\", self.P.shape)\n","        # print(\"P\", type(self.P))\n","        \n","        # Rigid transform from Velodyne coord to reference camera coord\n","        self.V2C = self.calibs[\"Tr_velo_to_cam\"].reshape(3, 4)\n","        # print(\"V2C\", self.V2C.shape)\n","        # print(\"V2C\", type(self.V2C))\n","\n","        # Rotation from reference camera coord to rect camera coord\n","        self.R0 = self.calibs[\"R0_rect\"].reshape(3, 3)\n","        # print(\"R0\", self.R0.shape)\n","        #¬†print(\"R0\", type(self.R0))\n","\n","    def print_calibs_file(self):\n","        for name, matrix in self.calibs.items():\n","            print(name, end= \":\")\n","            print(matrix)\n","            \n","    def read_calib_file(self, filepath):\n","        \"\"\" Read in a calibration file and parse into a dictionary.\n","        Ref: https://github.com/utiasSTARS/pykitti/blob/master/pykitti/utils.py\n","        \"\"\"\n","        data = {}\n","        with open(filepath, \"r\") as f:\n","            for line in f.readlines():\n","                line = line.rstrip()\n","                if len(line) == 0:\n","                    continue\n","                key, value = line.split(\":\", 1)\n","                # The only non-float values in these files are dates, which\n","                # we don't care about anyway\n","                try:\n","                    data[key] = np.array([float(x) for x in value.split()])\n","                except ValueError:\n","                    pass\n","        return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aZWuxKSFFEDA"},"source":["lidar2cam = LiDAR2Camera(calib_files[index])\n","print(\"P :\"+str(lidar2cam.P))\n","print(\"-\")\n","print(\"RO \"+str(lidar2cam.R0))\n","print(\"-\")\n","print(\"Velo 2 Cam \" +str(lidar2cam.V2C))\n","print(\"-\")\n","#print(\"Cam 2 Velo\" + str(lidar2cam.C2V))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.2 - Project the Points in the Image"],"metadata":{"id":"IFCEj5T0bRel"}},{"cell_type":"markdown","metadata":{"id":"zZ4qEcG9CNJq"},"source":["The main formula we'll use will be as follows:<p>\n","**Y(2D) = P x R0 x R|t x X (3D)** \n","\n","However, when looking at the dimensions:\n","\n","*   P: [3x4]\n","*   R0: [3x3]\n","*   R|t = Velo2Cam: [3x4]\n","*   X: [3x1]\n","\n","We'll need to convert some points into Homogeneous Coordinates:\n","* RO must go from 3x3 to 4x3\n","* x must go from 3x1 to 4x1\n","\n","Then, to retrieve the cartesian system, we'll divide as explained in the course."]},{"cell_type":"code","metadata":{"id":"-jB2aXeITxS8"},"source":["def cart2hom(self, pts_3d):\n","    \"\"\" Input: nx3 points in Cartesian\n","        Oupput: nx4 points in Homogeneous by pending 1\n","    \"\"\"\n","    n = pts_3d.shape[0]\n","    pts_3d_hom = np.hstack((pts_3d, np.ones((n, 1))))\n","    return pts_3d_hom\n","\n","LiDAR2Camera.cart2hom = cart2hom"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PUKxLc5HFsd"},"source":["def project_velo_to_ref(self, pts_3d_velo):\n","    pts_3d_velo = self.cart2hom(pts_3d_velo)  # nx4\n","    return np.dot(pts_3d_velo, np.transpose(self.V2C))\n","\n","LiDAR2Camera.project_velo_to_ref = project_velo_to_ref\n","\n","print(points[:1,:])\n","print(lidar2cam.project_velo_to_ref(points[:1,:]))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"siG14wbNcVCK"},"source":["def project_velo_to_image(self, pts_3d_velo):\n","    '''\n","    Input: 3D points in Velodyne Frame [nx3]\n","    Output: 2D Pixels in Image Frame [nx2]\n","\n","    Go from the point in 3D and convert it to 2D\n","    '''\n","    # Convert X to homogenous\n","    pts_3d_hom = self.cart2hom(pts_3d_velo)\n","    pt_3d = pts_3d_hom[0]\n","    \n","    # Convert V2C to 4x4\n","    m1 = self.V2C.shape[1]\n","    extra_matrix = np.zeros((1, m1))\n","    extra_matrix[:, -1] = 1\n","    R_t_homo = np.vstack((self.V2C, extra_matrix))\n","    \n","    R_t_X = np.matmul(R_t_homo, pts_3d_hom.transpose())\n","\n","    # Convert R0 to homogenous (4x4)\n","    m1 = self.R0.shape[1]\n","    extra_matrix = np.zeros((1, m1))\n","    R0_hom = np.vstack((self.R0, extra_matrix)) # Add one row of zeros\n","    m2 = m1 + 1\n","    extra_matrix = np.zeros((m2, 1))\n","    extra_matrix[-1, :] = 1\n","    R0_hom = np.hstack((R0_hom, extra_matrix)) # Add one column of zeros except last one\n","\n","    R0_R_t_X = np.matmul(R0_hom, R_t_X)\n","    homo_result = np.matmul(self.P, R0_R_t_X)\n","\n","    # Convert coordinates from homogenous to cartesian\n","    pts_2d = homo_result.transpose()\n","    pts_2d[:, 0] /= pts_2d[:, 2]\n","    pts_2d[:, 1] /= pts_2d[:, 2]\n","    return pts_2d[:, :-1]\n","    \n","    \"\"\"\n","    # OTHER TECHNIQUE\n","    R0_homo = np.vstack([self.R0, [0, 0, 0]])\n","    R0_homo_2 = np.column_stack([R0_homo, [0, 0, 0, 1]])\n","    p_r0 = np.dot(self.P, R0_homo_2) #PxR0\n","    p_r0_rt =  np.dot(p_r0, np.vstack((self.V2C, [0, 0, 0, 1]))) #PxROxRT\n","    pts_3d_homo = np.column_stack([pts_3d_velo, np.ones((pts_3d_velo.shape[0],1))])\n","    p_r0_rt_x = np.dot(p_r0_rt, np.transpose(pts_3d_homo))#PxROxRTxX\n","    pts_2d = np.transpose(p_r0_rt_x)\n","    \n","    pts_2d[:, 0] /= pts_2d[:, 2]\n","    pts_2d[:, 1] /= pts_2d[:, 2]\n","    return pts_2d[:, 0:2]\n","    \"\"\"\n","\n","LiDAR2Camera.project_velo_to_image = project_velo_to_image\n","print(points[:5,:3])\n","print(\"Euclidean Pixels\")\n","print(lidar2cam.project_velo_to_image(points[:5,:3]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F0NSdCLpCgBP"},"source":["### 1.3 - LiDAR in Image Field Of View"]},{"cell_type":"code","source":["def get_lidar_in_image_fov(self, pc_velo, xmin, ymin, xmax, ymax, return_more=False, clip_distance=2.0):\n","    \"\"\" \n","    Filter lidar points, keep those in image FOV\n","    \n","    pc_velo: 3D points in Velodyne Frame\n","    pts_2d: 2D Pixels in Image Frame\n","    fov_inds: row indices of valid points\n","    self.imgfov_pc_velo: filtered 2D Pixels in Image Frame\n","    \"\"\"\n","    pts_2d = self.project_velo_to_image(pc_velo)\n","\n","    # Remove the points outside the image\n","    a = [i for i, x in enumerate(pts_2d[:, 0] >= xmin) if x]\n","    b = [i for i, x in enumerate(pts_2d[:, 0] < xmax) if x]\n","    c = [i for i, y in enumerate(pts_2d[:, 1] >= ymin) if y]\n","    d = [i for i, y in enumerate(pts_2d[:, 1] < ymax) if y]\n","    # Remove points that are closer than the clip distance (2m)\n","    e =  [i for i, z in enumerate(pc_velo[:, 0] > clip_distance) if z]\n","    fov_inds = list(set(a) & set(b) & set(c) & set(d) & set(e))\n","\n","    self.imgfov_pc_velo = pc_velo[fov_inds, :]\n","    if return_more:\n","        return self.imgfov_pc_velo, pts_2d, fov_inds\n","    else:\n","        return self.imgfov_pc_velo\n","    \n","LiDAR2Camera.get_lidar_in_image_fov = get_lidar_in_image_fov\n","a, b, c = lidar2cam.get_lidar_in_image_fov(points[:5,:3], 0, 0, img.shape[1], img.shape[0], True)\n","print(a)\n","print(b)\n","print(c)"],"metadata":{"id":"CH51s8upHFgq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hVNFq-3gCnmL"},"source":["### 1.4 -- Get the LiDAR Points in Pixels"]},{"cell_type":"code","metadata":{"id":"1DOf4AXe8Kqf"},"source":["import cv2\n","\n","def show_lidar_on_image(self, pc_velo, img, debug=\"False\"):\n","    \"\"\" Project LiDAR points to image \n","    \n","    pc_velo: 3D points in Velodyne Frame\n","    cmap: colour map\n","    img: image\n","    self.imgfov_pts_2d: lidar points detected\n","    \"\"\"\n","    imgfov_pc_velo, pts_2d, fov_inds = self.get_lidar_in_image_fov(\n","        pc_velo, 0, 0, img.shape[1], img.shape[0], True\n","    )\n","    if (debug==True):\n","        print(\"3D PC Velo \"+ str(imgfov_pc_velo)) # The 3D point Cloud Coordinates\n","        print(\"2D PIXEL: \" + str(pts_2d)) # The 2D Pixels\n","        print(\"FOV : \"+str(fov_inds)) # Whether the Pixel is in the image or not\n","    self.imgfov_pts_2d = pts_2d[fov_inds, :]\n","\n","    cmap = plt.cm.get_cmap(\"hsv\", 256)\n","    cmap = np.array([cmap(i) for i in range(256)])[:, :3] * 255\n","    self.imgfov_pc_velo = imgfov_pc_velo\n","\n","    for i in range(self.imgfov_pts_2d.shape[0]):\n","        x = round(self.imgfov_pts_2d[i][0])\n","        y = round(self.imgfov_pts_2d[i][1])\n","\n","        depth = imgfov_pc_velo[i][0]\n","        r = 2\n","        colour = cmap[int(510/depth), :]\n","        thickness = -1\n","        img = cv2.circle(img, (x, y), radius=r, color=tuple(colour), thickness=-1)\n","        \n","    return img\n","\n","LiDAR2Camera.show_lidar_on_image = show_lidar_on_image\n","img_3 = image.copy()\n","img_3 = lidar2cam.show_lidar_on_image(points[:,:3], img_3)\n","plt.figure(figsize=(14,7))\n","plt.imshow(img_3)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AaLfy002oOIG"},"source":["#img_3 = lidar2cam.show_lidar_on_image(points[:,:3], image)\n","img_3 = image.copy()\n","img_3 = lidar2cam.show_lidar_on_image(points[:,:3], img_3)\n","plt.figure(figsize=(14,7))\n","plt.imshow(img_3)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WmYX4Iex6JzH"},"source":["## 2 - Detect Obstacles in 2D"]},{"cell_type":"markdown","source":["#### YOLO algorithm"],"metadata":{"id":"AC1x8H7MbA8M"}},{"cell_type":"code","metadata":{"id":"UTEnwwS47BcJ"},"source":["!python3 -m pip install yolov4==2.0.2 # After Checking, YOLO 2.0.2 works without modifying anything. Otherwise keep 1.2.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o-Py5K64gDvQ"},"source":["from yolov4.tf import YOLOv4\n","import tensorflow as tf\n","import time\n","\n","yolo = YOLOv4(tiny=False)\n","\n","yolo.classes = work_dir + \"/Yolov4/coco.names\"\n","yolo.make_model()\n","yolo.load_weights(work_dir + \"/Yolov4/yolov4.weights\", weights_type=\"yolo\")\n","\n","def run_obstacle_detection(img):\n","    start_time=time.time()\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    resized_image = yolo.resize_image(img)\n","    # Nomalising the image: 0 ~ 255 to 0.0 ~ 1.0\n","    resized_image = resized_image / 255.\n","    # input_data == Dim(1, input_size, input_size, channels)\n","    input_data = resized_image[np.newaxis, ...].astype(np.float32)\n","\n","    candidates = yolo.model.predict(input_data)\n","\n","    _candidates = []\n","    result = img.copy()\n","    for candidate in candidates:\n","        batch_size = candidate.shape[0]\n","        grid_size = candidate.shape[1]\n","        _candidates.append(tf.reshape(candidate, shape=(1, grid_size * grid_size * 3, -1)))\n","        #candidates == Dim(batch, candidates, (bbox))\n","        candidates = np.concatenate(_candidates, axis=1)\n","        #pred_bboxes == Dim(candidates, (x, y, w, h, class_id, prob))\n","        pred_bboxes = yolo.candidates_to_pred_bboxes(candidates[0], iou_threshold=0.35, score_threshold=0.40)\n","        pred_bboxes = pred_bboxes[~(pred_bboxes==0).all(1)] #https://stackoverflow.com/questions/35673095/python-how-to-eliminate-all-the-zero-rows-from-a-matrix-in-numpy?lq=1\n","        pred_bboxes = yolo.fit_pred_bboxes_to_original(pred_bboxes, img.shape)\n","        exec_time = time.time() - start_time\n","        # print(\"time: {:.2f} ms\".format(exec_time * 1000))\n","        #¬†print(pred_bboxes)\n","        result = yolo.draw_bboxes(img, pred_bboxes)\n","        result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n","        break\n","    return result, pred_bboxes\n","\n","result, pred_bboxes = run_obstacle_detection(image)\n","\"\"\"print(\"pred_bboxes\")\n","print(pred_bboxes)\n","print(\"result\")\n","print(result)\"\"\"\n","\n","fig_camera = plt.figure(figsize=(14, 7))\n","ax_lidar = fig_camera.subplots()\n","ax_lidar.imshow(result)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XJFi74VA7F0F"},"source":["## 3 - Fuse Points Clouds & Bounding Boxes"]},{"cell_type":"code","metadata":{"id":"3q6V1HE265jk"},"source":["lidar_img_with_bboxes = yolo.draw_bboxes(img_3, pred_bboxes)\n","fig_fusion = plt.figure(figsize=(14, 7))\n","ax_fusion = fig_fusion.subplots()\n","ax_fusion.imshow(lidar_img_with_bboxes)\n","plt.show()\n","cv2.imwrite(\"output/lidar_bboxes.png\", lidar_img_with_bboxes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Examples of outliers"],"metadata":{"id":"caKKjXmFPJxr"}},{"cell_type":"markdown","source":["**We'll see a few ways to filter outliers.** <p>"],"metadata":{"id":"bK3LDGwkIZoP"}},{"cell_type":"markdown","metadata":{"id":"q0Qebe-FHViZ"},"source":["Outliers are the points that belong to the bounding box, but not to the object.<p>\n","Here's an example of outliers:<p>\n","![outlier image](https://i.ibb.co/Fg0KV3k/Screenshot-2021-05-31-at-22-31-29.png)\n","\n","In this image, the points belong to the truck, but are also counted as part of the car."]},{"cell_type":"markdown","source":["**Shrink Factor** <p>\n","\n","The first technique we can use for that is a shrink factor.\n","Instead of considering the whole bounding box, we're considering only a part of it. A common choice is 10-15% shrinking.\n","![image_shrinks](https://i.ibb.co/Zcgzz6F/Screenshot-2021-05-31-at-22-45-36.png)"],"metadata":{"id":"wTK-5CgKIeDC"}},{"cell_type":"markdown","source":["#### Helper functions"],"metadata":{"id":"QG_gIHSaPNy_"}},{"cell_type":"markdown","source":["##### Point in bboxes"],"metadata":{"id":"lAdO7BTfaZUz"}},{"cell_type":"code","metadata":{"id":"k9VW-osNGbpe"},"source":["def rectContains(rect, pt, w, h, shrink_factor = 0):\n","    \"\"\"\n","    Returns whether the point is belongs to a bbox with shrinking.\n","\n","    rect: bounding box (x, y, w, h, class_id, probability)\n","    pt: point\n","    w: width of image\n","    h: height of image\n","    \"\"\"\n","    max_x = int(rect[0]*w + rect[2]*(1-shrink_factor)*(w/2))\n","    min_x = int(rect[0]*w - rect[2]*(1-shrink_factor)*(w/2))\n","    max_y = int(rect[1]*h + rect[3]*(1-shrink_factor)*(h/2))\n","    min_y = int(rect[1]*h - rect[3]*(1-shrink_factor)*(h/2))\n","\n","    if min_x <= pt[0] <= max_x:\n","        if min_y <= pt[1] <= max_y:\n","            \n","            return True\n","    return False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DewS97_cLjxJ"},"source":["**The second way will be through outlier removal techniques, such as: 3 Sigma, RANSAC, and etc. <p>**\n"]},{"cell_type":"markdown","source":["##### One Sigma"],"metadata":{"id":"Z9Huy-uyakTW"}},{"cell_type":"code","metadata":{"id":"bk6HfEgAsg9R"},"source":["import statistics\n","import random\n","\n","def filter_outliers(distances):\n","    \"\"\"Remove outliers using by removing distances further away than one sigma\"\"\"\n","    mean = np.mean(distances)\n","    std = np.std(distances)\n","\n","    return np.array(list({x for i, x in enumerate(distances) if x < mean + std} & \n","                {x for i, x in enumerate(distances) if x > mean - std}))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Best Distances"],"metadata":{"id":"yFpIyRjeapUu"}},{"cell_type":"code","metadata":{"id":"MgCLQn9HM39S"},"source":["def get_best_distance(distances, technique=\"closest\"):\n","    \"\"\" \n","    Get the best distance according to at least 3 criterias (closest, average, median, farthest)\n","    \"\"\"\n","    if technique==\"closest\":\n","        return np.amin(distances)\n","    if technique==\"average\":\n","        return np.mean(distances)\n","    if technique==\"median\":\n","        return np.median(distances)\n","    if technique==\"farthest\":\n","        return np.amax(distances)\n","    else:\n","        return np.median(distances)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Fuse boxes and points"],"metadata":{"id":"sDcyYc7LauC0"}},{"cell_type":"code","metadata":{"id":"JF8wnxMuj_zQ"},"source":["def lidar_camera_fusion(self, pred_bboxes, image, dist_technique=\"average\"):\n","    \"\"\"\n","    Implements the fusion between boxes and points.\n","    \n","    pred_bboxes: bounding boxes\n","    image: image from obstacle detection\n","    distances_obj: best distances for each bbox\n","    \"\"\"\n","    img_bis = image.copy()\n","\n","    cmap = plt.cm.get_cmap(\"hsv\", 256)\n","    cmap = np.array([cmap(i) for i in range(256)])[:, :3] * 255\n","    distances_obj = []\n","\n","    # Go through every box detected by YOLOv4\n","    for box in pred_bboxes:\n","        distances = []\n","        # For every box, loop through all the points detected\n","        for i in range(self.imgfov_pts_2d.shape[0]):\n","            depth = self.imgfov_pc_velo[i][0]\n","            if rectContains(box, self.imgfov_pts_2d[i], image.shape[1], image.shape[0], shrink_factor=0.2):\n","                # point belongs to the box\n","                distances.append(depth)\n","                # draw the points\n","                x = round(self.imgfov_pts_2d[i][0])\n","                y = round(self.imgfov_pts_2d[i][1])\n","\n","                r = 2\n","                colour = cmap[int(510/depth), :]\n","                thickness = -1\n","                cv2.circle(img_bis, (x, y), radius=r, color=tuple(colour), thickness=-1)\n","        h, w, _ = img_bis.shape\n","        if (len(distances)>2):\n","            distances = filter_outliers(distances)\n","            dist = get_best_distance(distances, technique=\"average\") \n","            distances_obj.append(dist)               \n","            cv2.putText(img_bis, '{0:.2f} m'.format(dist), (int(box[0]*w), int(box[1]*h)), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 0), 3, cv2.LINE_AA)\n","    \n","    return img_bis, distances_obj\n","\n","LiDAR2Camera.lidar_camera_fusion = lidar_camera_fusion"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T6w7bFcYk03p"},"source":["final_result, _ = lidar2cam.lidar_camera_fusion(pred_bboxes, result)\n","\n","fig_keeping = plt.figure(figsize=(14, 7))\n","ax_keeping = fig_keeping.subplots()\n","ax_keeping.imshow(final_result)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PlrlH991R34T"},"source":["### Build a Pipeline"]},{"cell_type":"code","metadata":{"id":"FWhXl-ftiIA6"},"source":["def pipeline(self, image, point_cloud, dist_technique=\"average\"):\n","    \"For a pair of 2 Calibrated Images\"\n","    img = image.copy()\n","    # Show LidAR on Image\n","    lidar_img = self.show_lidar_on_image(point_cloud[:,:3], image)\n","    # Run obstacle detection in 2D\n","    result, pred_bboxes = run_obstacle_detection(img)\n","    # Fuse Point Clouds & Bounding Boxes\n","    img_final, _ = self.lidar_camera_fusion(pred_bboxes, result, dist_technique)\n","    return img_final\n","\n","LiDAR2Camera.pipeline = pipeline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dhtJIYtTGLX1"},"source":["image_files = sorted(glob.glob(work_dir + \"/data/img/*.png\"))\n","point_files = sorted(glob.glob(work_dir + \"/data/velodyne/*.pcd\"))\n","label_files = sorted(glob.glob(work_dir + \"/data/label/*.txt\"))\n","calib_files = sorted(glob.glob(work_dir + \"/data/calib/*.txt\"))\n","\n","lidar2cam = LiDAR2Camera(calib_files[index])\n","cloud = o3d.io.read_point_cloud(pcd_file)\n","points= np.asarray(cloud.points)\n","\n","index = 0\n","image = cv2.cvtColor(cv2.imread(image_files[index]), cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(14,7))\n","final_result = lidar2cam.pipeline(image.copy(), points, dist_technique=\"furthest\")\n","plt.imshow(final_result)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OAnmN-M6z5o_"},"source":["## Comparing with the Ground Truth\n"]},{"cell_type":"markdown","source":["We want to see how the bounding boxes that we calculate compare with the true bounding boxes so we can choose the proper `dist_technique`."],"metadata":{"id":"jobLEVzdJb-0"}},{"cell_type":"code","metadata":{"id":"r07ai06Pz7TT"},"source":["image_gt = image.copy()\n","\n","with open(label_files[index], 'r') as f:\n","    fin = f.readlines()\n","    for line in fin:\n","        if line.split(\" \")[0] != \"DontCare\":\n","            #print(line)\n","            x1_value = int(float(line.split(\" \")[4]))\n","            y1_value = int(float(line.split(\" \")[5]))\n","            x2_value = int(float(line.split(\" \")[6]))\n","            y2_value = int(float(line.split(\" \")[7]))\n","            dist = float(line.split(\" \")[13])\n","            cv2.rectangle(image_gt, (x1_value, y1_value), (x2_value, y2_value), (0,205,0), 10)\n","            cv2.putText(image_gt, str(dist), (int((x1_value+x2_value)/2),int((y1_value+y2_value)/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2, cv2.LINE_AA)    \n","\n","f, (ax1, ax2) = plt.subplots(1, 2, figsize=(30,20))\n","ax1.imshow(image_gt)\n","ax1.set_title('Ground Truth', fontsize=30)\n","ax2.imshow(final_result) # or flag\n","ax2.set_title('Prediction', fontsize=30)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6eZq-Put27fl"},"source":["## Shooting a Portfolio Video"]},{"cell_type":"code","metadata":{"id":"VieqzsWS2-Wk"},"source":["import glob\n","num_videos = 4\n","\n","for n in range(1, num_videos):\n","    n = str(n + 1)\n","\n","    video_images = sorted(glob.glob(work_dir + \"/data/videos/video_\" + n + \"/images/*.png\"))\n","    video_points = sorted(glob.glob(work_dir + \"/data/videos/video_\" + n + \"/points/*.pcd\"))\n","\n","    # Build a LiDAR2Cam object\n","    lidar2cam_video = LiDAR2Camera(calib_files[0])\n","\n","    result_video = []\n","    for idx, img in enumerate(video_images):\n","        image = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB)\n","        point_cloud = np.asarray(o3d.io.read_point_cloud(video_points[idx]).points)\n","        result_video.append(lidar2cam_video.pipeline(image, point_cloud))\n","\n","    name = work_dir + '/output/early_fusion_'+ n +'.mp4'\n","    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n","    out = cv2.VideoWriter(name, fourcc, 15, (image.shape[1],image.shape[0]))\n","    \n","    for i in range(len(result_video)):\n","        out.write(cv2.cvtColor(result_video[i], cv2.COLOR_BGR2RGB))\n","        #out.write(result_video[i])\n","    out.release()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hpE17S2vjP8w"},"source":[],"execution_count":null,"outputs":[]}]}